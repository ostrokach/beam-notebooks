{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYniTY18yHx2"
   },
   "source": [
    "# New York Taxirides to Events\n",
    "\n",
    "Parse a dataset of New York Taxirides into start and stop events to be used for streaming Beam demos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5zc3pwq0smN-"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwceiH15eVt-"
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import itertools\n",
    "import os\n",
    "import os.path as op\n",
    "import shutil\n",
    "import time\n",
    "import uuid\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyproj\n",
    "import pytz\n",
    "import requests\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_columns\", 100)\n",
    "pd.set_option(\"max_rows\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hzzEnIEftMYZ"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_NAME = \"new_york_taxirides_to_events\"\n",
    "try:\n",
    "    os.mkdir(NOTEBOOK_NAME)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS = [\n",
    "    \"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-12.csv\",\n",
    "    \"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-01.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "except Exception:\n",
    "    print(\"No autoreload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Create events dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load New York taxirides data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, outfile):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        with open(outfile, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for url in URLS:\n",
    "    output_file = os.path.join(NOTEBOOK_NAME, url.split(\"/\")[-1])\n",
    "    try:\n",
    "        df = pd.read_csv(output_file)\n",
    "    except IOError:\n",
    "        download_file(URL, output_file)\n",
    "        df = pd.read_csv(output_file)\n",
    "    dfs.append(df)\n",
    "        \n",
    "new_york_taxirides = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_york_taxirides.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(new_york_taxirides) > (0.7 * len(new_york_taxirides[[\"pickup_longitude\", \"pickup_latitude\"]].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(new_york_taxirides) > (0.7 * len(new_york_taxirides[[\"dropoff_longitude\", \"dropoff_latitude\"]].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `events_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_events = len(new_york_taxirides)\n",
    "\n",
    "events = {\n",
    "    \"event_type\": np.array([\"start\"] * num_events + [\"stop\"] * num_events),\n",
    "    # TODO(ostrokach): this should be a pyarrow timestamp type.\n",
    "    \"timestamp\": np.r_[\n",
    "        new_york_taxirides[\"tpep_pickup_datetime\"],\n",
    "        new_york_taxirides[\"tpep_dropoff_datetime\"],\n",
    "    ],\n",
    "    \"longitude\": np.r_[\n",
    "        new_york_taxirides[\"pickup_longitude\"], new_york_taxirides[\"dropoff_longitude\"]\n",
    "    ],\n",
    "    \"latitude\": np.r_[\n",
    "        new_york_taxirides[\"pickup_latitude\"], new_york_taxirides[\"dropoff_latitude\"]\n",
    "    ],\n",
    "    \"trip_miles\": np.r_[\n",
    "        np.array([np.nan] * num_events), new_york_taxirides[\"trip_distance\"]\n",
    "    ],\n",
    "    \"trip_total\": np.r_[\n",
    "        np.array([np.nan] * num_events), new_york_taxirides[\"total_amount\"]\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = pd.DataFrame(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_milliseconds(timestamp_str):\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "\n",
    "    dt = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = dt.replace(tzinfo=pytz.UTC)  # .astimezone(pytz.timezone('America/Chicago'))\n",
    "    unix_dt = datetime.utcfromtimestamp(0).replace(tzinfo=pytz.UTC)\n",
    "    dt_delta = int((dt - unix_dt).total_seconds() * 1000)\n",
    "    return dt_delta\n",
    "\n",
    "timestamp_to_milliseconds(\"2016-01-01 00:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with concurrent.futures.ProcessPoolExecutor() as p:\n",
    "    events_df[\"timestamp_milliseconds\"] = list(\n",
    "        tqdm.tqdm_notebook(\n",
    "            p.map(\n",
    "                timestamp_to_milliseconds,\n",
    "                (ts for ts in events_df[\"timestamp\"].values),\n",
    "                chunksize=1000,\n",
    "            ),\n",
    "            total=len(events_df),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Mercator coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geographic_to_utm(longitude, latitude, _cache={}):\n",
    "    if (longitude, latitude) in _cache:\n",
    "        return _cache[(longitude, latitude)]\n",
    "\n",
    "    from pyproj import Proj, transform\n",
    "\n",
    "    x, y = transform(\n",
    "        Proj(init=\"epsg:4326\"), Proj(init=\"epsg:3857\"), longitude, latitude\n",
    "    )\n",
    "\n",
    "    _cache[(longitude, latitude)] = (x, y)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "geographic_to_utm(-87.632746, 41.880994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_major = 6378137.000\n",
    "\n",
    "events_df[\"utm_x\"] = (r_major * 2 * np.pi / 360) * events_df[\"longitude\"]\n",
    "scale = events_df[\"utm_x\"] / events_df[\"longitude\"]\n",
    "events_df[\"utm_y\"] = (\n",
    "    180.0 / np.pi * np.log(np.tan((np.pi / 4.0) + events_df[\"latitude\"] * (np.pi / 180.0 / 2.0))) * scale\n",
    ")\n",
    "events_df[\"utm_y\"] = events_df[\"utm_y\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "for row in itertools.islice(events_df.itertuples(), 100):\n",
    "    utm_x, utm_y = geographic_to_utm(row.longitude, row.latitude)\n",
    "    np.testing.assert_almost_equal(utm_x, row.utm_x)\n",
    "    np.testing.assert_almost_equal(utm_y, row.utm_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort table by timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = events_df.sort_values(\"timestamp\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter to New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERCATOR_X_RANGE = (-8240000, -8220000)\n",
    "\n",
    "fg, ax = plt.subplots()\n",
    "_ = ax.hist(\n",
    "    np.clip(events_df[\"utm_x\"], *MERCATOR_X_RANGE), bins=100, range=MERCATOR_X_RANGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERCATOR_Y_RANGE = (4950000, 5000000)\n",
    "\n",
    "fg, ax = plt.subplots()\n",
    "_ = ax.hist(\n",
    "    np.clip(events_df[\"utm_y\"], *MERCATOR_Y_RANGE), bins=100, range=MERCATOR_Y_RANGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_df = events_df[\n",
    "#     (MERCATOR_X_RANGE[0] <= events_df[\"utm_x\"])\n",
    "#     & (MERCATOR_X_RANGE[1] > events_df[\"utm_x\"])\n",
    "#     & (MERCATOR_Y_RANGE[0] <= events_df[\"utm_y\"])\n",
    "#     & (MERCATOR_Y_RANGE[1] > events_df[\"utm_y\"])\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df_filtered = events_df[\n",
    "    (events_df[\"timestamp\"] != \"1900-01-01 00:00:00\") &\n",
    "    (events_df[\"longitude\"] != 0) &\n",
    "    (events_df[\"latitude\"] != 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = op.join(NOTEBOOK_NAME, \"new-york-taxi-events.parquet\")\n",
    "print(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(events_df_filtered, preserve_index=False)\n",
    "pq.write_table(table, output_file, row_group_size=20000, flavor=\"spark\", version=\"2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = pq.ParquetFile(output_file)\n",
    "pt.num_row_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.read_row_group(0).to_pandas(integer_object_nulls=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "5zc3pwq0smN-"
   ],
   "name": "streambasedcache.ipynb",
   "provenance": [
    {
     "file_id": "14kD7KIsqpkuyVlvIKa4GUCOFXsMpiouh",
     "timestamp": 1561506409299
    },
    {
     "file_id": "1ZShzPcvbGP5mNaVb5xcjvR6r_QBkFyd6",
     "timestamp": 1561505940600
    },
    {
     "file_id": "https://github.com/ostrokach/beam-notebooks/blob/master/feature/filebasedcache.ipynb",
     "timestamp": 1561071440262
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
